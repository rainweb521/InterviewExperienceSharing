

记录下来自己面试的一些问题和经验，期望以后能帮到别人。

[TOC]



# Java篇

### 谈谈你对Java平台的理解

说出Java的平台无关性，引出JVM虚拟机，JVM的好处和带来的内存管理问题，以及GC机制，Java内存结构

面向对象编程，从封装，继承，多态，反射，Java提供了丰富的类库，IO/NIO，网络，utils，更广阔的还有Java生态，广泛的应用场景。



## 集合类

### hashmap的数据结构和死锁的生成

HashMap 是我们经常会用到的集合类，JDK 1.7 之前底层使用了数组加链表的组合结构，新添加的元素通过取模的方式，定位 Table 数组位置，然后将元素加入链表头部，这样下次提取时就可以快速被访问到。访问数据时，也是通过取模的方式，定位数组中的位置，然后再遍历链表，依次比较，获取相应的元素。

如果 HasMap 中元素过多时，可能导致某个位置上链表很长。原本 O(1) 查找性能，可能就退化成 O(N),严重降低查找效率。为了避免这种情况，当 HasMap 元素数量满足以下条件时，将会自动扩容,重新分配元素。

```
// size:HashMap 中实际元素数量
//capacity：HashMap 容量，即 Table 数组长度，默认为：16
//loadFactor：负载因子，默认为：0.75
size>=capacity*loadFactor
```

HasMap 将会把容量扩充为原来的两倍，然后将原数组元素迁移至新数组。

 旧数组元素迁移到新数组时，依旧采用『头插入法』，这样将会导致新链表元素的逆序排序。

多线程并发扩容的情况下，链表可能形成死链（环形链表）。一旦有任何查找元素的动作，线程将会陷入死循环,从而引发 CPU 使用率飙升。

***\*1. JDK1.8 改进方案\****

JDK1.8 HashMap 底层结构进行彻底重构，使用数组加链表/红黑树方式这种组合结构。

![image-20200911084956930](assets/image-20200911084956930.png)

新元素依旧通过取模方式获取 Table 数组位置，然后再将元素加入链表尾部。一旦链表元素数量超过 8 之后，自动转为红黑树，进一步提高了查找效率。

由于 JDK1.8 链表采用『尾插入』法，从而避免并发扩容情况下链表形成死链的可能。

### **SynchronizedMap**和**ConcurrentHashMap**

对于并发的情况，我们可以使用 JDK 提供 SynchronizedMap 保证安全。

SynchronizedMap 是一个内部类，只能通过以下方式创建实例。

Map m = Collections.synchronizedMap(**new** HashMap(...));

每个方法内将会使用 synchronized 关键字加锁，从而保证并发安全。

由于多线程共享同一把锁，导致同一时间只允许一个线程读写操作，其他线程必须等待，极大降低的性能。

并且大多数业务场景都是读多写少，多线程读操作本身并不冲突，SynchronizedMap 极大的限制读的性能。

所以多线程并发场景我们很少使用 SynchronizedMap。

**ConcurrentHashMap**

既然多线程共享一把锁，导致性能下降。那么设想一下我们是不是多搞几把锁，分流线程，减少锁冲突，提高并发度。

ConcurrentHashMap 正是使用这种方法，不但保证并发过程数据安全，又保证一定的效率。

**1. JDK1.7**

JDK1.7  ConcurrentHashMap 数据结构如下所示:

![image-20200911085440423](assets/image-20200911085440423.png)

Segament 是一个ConcurrentHashMap内部类，底层结构与 HashMap 一致。另外Segament 继承自 ReentrantLock

当新元素加入 ConcurrentHashMap 时，首先根据 key hash 值找到相应的 Segament。接着直接对 Segament 上锁，若获取成功，后续操作步骤如同 HashMap。

由于锁的存在，Segament 内部操作都是并发安全，同时由于其他 Segament 未被占用，因此可以支持 concurrencyLevel 个线程安全的并发读写。

***\*size 统计问题\****

虽然 ConcurrentHashMap 引入分段锁解决多线程并发的问题，但是同时引入新的复杂度，导致计算 ConcurrentHashMap 元素数量将会变得复杂。

由于 ConcurrentHashMap 元素实际分布在 Segament 中，为了统计实际数量，只能遍历 Segament 数组求和。

为了数据的准确性，这个过程过我们需要锁住所有的 Segament，计算结束之后，再依次解锁。不过这样做，将会导致写操作被阻塞，一定程度降低 ConcurrentHashMap 性能。

所以这里对 ConcurrentHashMap#size 统计方法进行一定的优化。

Segment 每次被修改（写入，删除）,都会对 modCount（更新次数）加 1。只要相邻两次计算获取所有的 Segment modCount 总和一致，则代表两次计算过程并无写入或删除，可以直接返回统计数量。

如果三次计算结果都不一致，那没办法只能对所有 Segment 加锁，重新计算结果。

这里需要注意的是，这里求得 size 数量不能做到 100% 准确。这是因为最后依次对 Segment 解锁后，可能会有其他线程进入写入操作。这样就导致返回时的数量与实际数不一致。

不过这也能被接受，总不能因为为了统计元素停止所有元素的写入操作。

***\*性能问题\****

想象一种极端情况的，所有写入都落在同一个 Segment中，这就导致ConcurrentHashMap 退化成 SynchronizedMap，共同抢一把锁。

***\*2. JDK1.8 改进方案\****

JDK1.8 之后，ConcurrentHashMap 取消了分段锁的设计，进一步减少锁冲突的发生。另外也引入红黑树的结构，进一步提高查找效率。

数据结构如下所示：

![image-20200911085804944](assets/image-20200911085804944.png)

Table 数组的中每一个 Node 我们都可以看做一把锁，这就避免了 Segament 退化问题。

另外一旦 ConcurrentHashMap 扩容， Table 数组元素变多，锁的数量也会变多，并发度也会提高。

总的来说，JDK1.8 使用 CAS 方法加 synchronized 方式，保证并发安全。

ConcurrentHashMap 通过使用分段锁的设计方式，降低锁的粒度，提高并发度。我们可以借鉴这种设计，解决某些热点数据更新问题。

SynchronizedMap 虽然线程安全，但是由于锁粒度太大，导致性能太低，所以也不太适合在多线程使用。

ConcurrentHashMap 由于使用多把锁，充分降低多线程并发竞争的概率，提高了并发度，非常适合在多线程中使用。

### Hashmap为什么这里使用红黑树？而不是其他二叉树呢？



## 多线程

### 为什么要使用线程池

合理的使用线程池，则可以避免把系统搞崩的窘境。总得来说，使用线程池可以带来以下几个好处：

1. 降低资源消耗。通过重复利用已创建的线程，降低线程创建和销毁造成的消耗。
2. 提高响应速度。当任务到达时，任务可以不需要等到线程创建就能立即执行。
3. 增加线程的可管理型。线程是稀缺资源，使用线程池可以进行统一分配，调优和监控。



### 线程池的运作流程

![image-20200911105737949](assets/image-20200911105737949.png)

### 线程池目前有5个状态：

- RUNNING：接受新任务并处理排队的任务。
- SHUTDOWN：不接受新任务，但处理排队的任务。
- STOP：不接受新任务，不处理排队的任务，并中断正在进行的任务。
- TIDYING：所有任务都已终止，workerCount 为零，线程转换到 TIDYING 状态将运行 terminated() 钩子方法。
- TERMINATED：terminated() 已完成。

![img](http://mmbiz.qpic.cn/sz_mmbiz_png/KRRxvqGcicZHzFS6POg6Tgicicv5IMtLibWjG81s8quLPrYdRaRV8DvJ5CU81DUldussXW4mKoSZEp86CiaHYcakZCg/640?wx_fmt=png&wxfrom=5&wx_lazy=1&wx_co=1)

### 线程池有哪些拒绝策略？

常见的有以下几种：

AbortPolicy：中止策略。默认的拒绝策略，直接抛出 RejectedExecutionException。调用者可以捕获这个异常，然后根据需求编写自己的处理代码。

DiscardPolicy：抛弃策略。什么都不做，直接抛弃被拒绝的任务。

DiscardOldestPolicy：抛弃最老策略。抛弃阻塞队列中最老的任务，相当于就是队列中下一个将要被执行的任务，然后重新提交被拒绝的任务。如果阻塞队列是一个优先队列，那么“抛弃最旧的”策略将导致抛弃优先级最高的任务，因此最好不要将该策略和优先级队列放在一起使用。

CallerRunsPolicy：调用者运行策略。在调用者线程中执行该任务。该策略实现了一种调节机制，该策略既不会抛弃任务，也不会抛出异常，而是将任务回退到调用者（调用线程池执行任务的主线程），由于执行任务需要一定时间，因此主线程至少在一段时间内不能提交任务，从而使得线程池有时间来处理完正在执行的任务。

### 在我们实际使用中，线程池的大小配置多少合适？

要想合理的配置线程池大小，首先我们需要区分任务是计算密集型还是I/O密集型。

对于计算密集型，设置 线程数 = CPU数 + 1，通常能实现最优的利用率。

对于I/O密集型，网上常见的说法是设置 线程数 = CPU数 * 2 ，这个做法是可以的，但个人觉得不是最优的。

在我们日常的开发中，我们的任务几乎是离不开I/O的，常见的网络I/O（RPC调用）、磁盘I/O（数据库操作），并且I/O的等待时间通常会占整个任务处理时间的很大一部分，在这种情况下，开启更多的线程可以让 CPU 得到更充分的使用，一个较合理的计算公式如下：

线程数 = CPU数 * CPU利用率 * (任务等待时间 / 任务计算时间 + 1)

例如我们有个定时任务，部署在4核的服务器上，该任务有100ms在计算，900ms在I/O等待，则线程数约为：4 * 1 * (1 + 900 / 100) = 40个。

### 解释执行和编译执行

解释执行：执行时逐条将字节码编译成机器码并执行，无需保存不占内存。

即时编译：将一个方法中包含的所有字节码编译成机器码后再执行，编译后的指令保存在内存中，比较占用内存，适合二八模式，热点代码用即时编译

![image-20200915083016220](assets/image-20200915083016220.png)

为了满足不同用户场景的需要,HotSpot内置了多个即时编译器:C1,C2和Graal

之所以引入多个即时编译器,是为了在编译时间和生成代码的执行效率之间进行取舍。C1又叫做Client编译器,面向的是对启动性能有要求的客户端GUI程序,采用的优化手段相对简单,因此编译时间较短。

C2又叫做Server编译器,面向的是对峰值性能有要求的服务器端程序,采用优化手段相对复杂,因此编译时间较长,但同时生成代码的执行效率较高。

从Java 7开始,HotSpot默认采用分层编译的方式:热点方法首先会被C1编译,而后热点方法中的热点会进一步被C2编译



### Netty，Kafka中的零拷贝技术

Java 进程发起 Read 请求加载数据大致的流程图：

![image-20200916083715980](assets/image-20200916083715980.png)

进程发起 Read 请求之后，内核接收到 Read 请求之后，会先检查内核空间中是否已经存在进程所需要的数据，如果已经存在，则直接把数据 Copy 给进程的缓冲区。如果没有内核随即向磁盘控制器发出命令，要求从磁盘读取数据，磁盘控制器把数据直接写入内核 Read 缓冲区，这一步通过 DMA 完成。

接下来就是内核将数据 Copy 到进程的缓冲区；如果进程发起 Write 请求，同样需要把用户缓冲区里面的数据 Copy 到内核的 Socket 缓冲区里面，然后再通过 DMA 把数据 Copy 到网卡中，发送出去。

- 零拷贝机制可以减少数据在内核缓冲区和用户进程缓冲区之间反复的 I/O 拷贝操作。
- 零拷贝机制可以减少用户进程地址空间和内核地址空间之间因为上下文切换而带来的 CPU 开销。

现代操作系统都使用虚拟内存，使用虚拟的地址取代物理地址，这样做的好处是：

- 一个以上的虚拟地址可以指向同一个物理内存地址。
- 虚拟内存空间可大于实际可用的物理地址。

利用第一条特性可以把内核空间地址和用户空间的虚拟地址映射到同一个物理地址，这样 DMA 就可以填充对内核和用户空间进程同时可见的缓冲区了。省去了内核与用户空间的往来拷贝

![image-20200916083917552](assets/image-20200916083917552.png)

在 Linux 中零拷贝技术主要有 3 个实现思路：用户态直接 I/O、减少数据拷贝次数以及写时复制技术。

- 用户态直接 I/O：应用程序可以直接访问硬件存储，操作系统内核只是辅助数据传输。这种方式依旧存在用户空间和内核空间的上下文切换，硬件上的数据直接拷贝至了用户空间，不经过内核空间。因此，直接 I/O 不存在内核空间缓冲区和用户空间缓冲区之间的数据拷贝。
- 减少数据拷贝次数：在数据传输过程中，避免数据在用户空间缓冲区和系统内核空间缓冲区之间的CPU拷贝，以及数据在系统内核空间内的CPU拷贝，这也是当前主流零拷贝技术的实现思路。
- 写时复制技术：写时复制指的是当多个进程共享同一块数据时，如果其中一个进程需要对这份数据进行修改，那么将其拷贝到自己的进程地址空间中，如果只是数据读取操作则不需要进行拷贝操作。

RocketMQ 选择了 mmap + write 这种零拷贝方式，适用于业务级消息这种小块文件的数据持久化和传输；而 Kafka 采用的是 sendfile 这种零拷贝方式，适用于系统日志消息这种高吞吐量的大块文件的数据持久化和传输。但是值得注意的一点是，Kafka 的索引文件使用的是 mmap + write 方式，数据文件使用的是 sendfile 方式。


链接：https://juejin.im/post/6844903949359644680



- kafka节点之间如何复制备份的？
- kafka消息是否会丢失？为什么？
- kafka最合理的配置是什么？
- kafka的leader选举机制是什么？
- kafka对硬件的配置有什么要求？
- kafka的消息保证有几种方式？
- kafka为什么会丢消息？